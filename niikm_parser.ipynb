{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "kt0gjvkrw4Fb"
      },
      "outputs": [],
      "source": [
        "import collections\n",
        "from bs4 import BeautifulSoup as bs\n",
        "import os\n",
        "import pandas as pd\n",
        "import re\n",
        "import requests\n",
        "import time\n",
        "import xml.etree.ElementTree\n",
        "\n",
        "st_accept = \"text/html\"\n",
        "st_useragent = \"Mozilla/5.0 (Macintosh; Intel Mac OS X 12_3_1) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/15.4 Safari/605.1.15\"\n",
        "headers = {\n",
        "   \"Accept\": st_accept,\n",
        "   \"User-Agent\": st_useragent\n",
        "}\n",
        "\n",
        "NIIKM_BASE_LINK = \"https://www.niikm.ru/\"\n",
        "NIIKM_LINKS = [\"products/azot/\"]\n",
        "PANDAS_CRUTCH_BASE = \"/content/temp\"\n",
        "REQUEST_DELAY = 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nGHxyKUfxbsg",
        "outputId": "3a2bb3fe-13f8-4e5e-8ded-33b3c0739ddf"
      },
      "outputs": [],
      "source": [
        "def parse_url(url):\n",
        "    print(\"Parsing html from \\\"\" + url + \"\\\".\")\n",
        "\n",
        "    req = requests.get(url, headers)\n",
        "\n",
        "    if req.status_code != 200:\n",
        "        print(\"    ERROR: The request status is \" + str(req.status_code) + \".\")\n",
        "        return\n",
        "\n",
        "    soup = bs(req.text, 'lxml')\n",
        "    #processed_tags = set()\n",
        "\n",
        "    #A crutch for Pandas library.\n",
        "    PANDAS_CRUTCH = PANDAS_CRUTCH_BASE + str(id(url)) + \".txt\"\n",
        "    temp_file = open(PANDAS_CRUTCH, 'w', encoding=\"utf-8\")\n",
        "    temp_file.write(req.text)\n",
        "    temp_file.close()\n",
        "    tables = pd.read_html(PANDAS_CRUTCH)\n",
        "    table_index = 0\n",
        "\n",
        "    page_title = soup.find(\"div\", attrs={\"class\":\"title-page\"})\n",
        "    if page_title == None:\n",
        "        print(\"    ERROR: Could not find page title.\")\n",
        "        return\n",
        "\n",
        "    f = open(page_title.text + \".txt\", 'w', encoding=\"utf-8\")\n",
        "    f.write(page_title.text.lstrip().rstrip().replace(\"\\n\",\"\") + \"\\n\")\n",
        "\n",
        "    #Required for distinguishing articles and product pages.\n",
        "    is_product_page = True\n",
        "    product_description = soup.find(\"div\", attrs={\"class\":\"product-card-dbl__col product-card-dbl__description-product\"})\n",
        "    if product_description == None:\n",
        "        is_product_page = False\n",
        "\n",
        "    if is_product_page:\n",
        "        print(\"    INFO: The page is a product page.\\n    Parsing product description...\")\n",
        "        for i in product_description.children:\n",
        "            if i.name == \"p\":\n",
        "                f.write(i.text.lstrip().rstrip() + \"\\n\")\n",
        "        print(\"        Done.\")\n",
        "\n",
        "        product_assortment = soup.find(\"div\", attrs={\"class\":\"product-card-dbl__product-range-wrap\"})\n",
        "        if product_assortment == None:\n",
        "            print(\"    INFO: The page does not contain references to the product subtypes.\")\n",
        "        else:\n",
        "            print(\"    Starting the recursive traversal of the product subtypes...\")\n",
        "            for i in product_assortment:\n",
        "                time.sleep(REQUEST_DELAY)\n",
        "\n",
        "                if i.name == \"a\":\n",
        "                    parse_url(NIIKM_BASE_LINK + i[\"href\"])\n",
        "\n",
        "            print(\"        Done.\")\n",
        "    else:\n",
        "        #TODO: add code for articles.\n",
        "        pass\n",
        "\n",
        "    f.close()\n",
        "    os.remove(PANDAS_CRUTCH)\n",
        "    print(\"    Done.\")\n",
        "\n",
        "for i in NIIKM_LINKS:\n",
        "    parse_url(NIIKM_BASE_LINK + i)\n",
        "    time.sleep(REQUEST_DELAY)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
