{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "kt0gjvkrw4Fb"
      },
      "outputs": [],
      "source": [
        "import collections\n",
        "from bs4 import BeautifulSoup as bs\n",
        "import os\n",
        "import pandas as pd\n",
        "import re\n",
        "import requests\n",
        "import time\n",
        "import xml.etree.ElementTree\n",
        "\n",
        "st_accept = \"text/html\"\n",
        "st_useragent = \"Mozilla/5.0 (Macintosh; Intel Mac OS X 12_3_1) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/15.4 Safari/605.1.15\"\n",
        "headers = {\n",
        "   \"Accept\": st_accept,\n",
        "   \"User-Agent\": st_useragent\n",
        "}\n",
        "\n",
        "NIIKM_BASE_LINK = \"https://www.niikm.ru/\"\n",
        "NIIKM_LINKS = [\"products/azot/\", \"products/argon/\", \"products/acetylene/\", \"products/hydrogen/\", \"products/helium/\",\n",
        "               \"products/carbon_dioxide/\", \"products/oxygen/\", \"products/krypton/\", \"products/xenon/\",\n",
        "               \"products/methane/\", \"products/neon/\", \"products/spbt/\"]\n",
        "PANDAS_CRUTCH_BASE = \"/content/temp\"\n",
        "REQUEST_DELAY = 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nGHxyKUfxbsg"
      },
      "outputs": [],
      "source": [
        "def parse_url(url, recursive_call):\n",
        "    print(\"Parsing html from \\\"\" + url + \"\\\".\")\n",
        "\n",
        "    req = requests.get(url, headers)\n",
        "\n",
        "    if req.status_code != 200:\n",
        "        print(\"    ERROR: The request status code is \" + str(req.status_code) + \".\")\n",
        "        return\n",
        "\n",
        "    soup = bs(req.text, 'lxml')\n",
        "    #processed_tags = set()\n",
        "\n",
        "    #A crutch for Pandas library.\n",
        "    PANDAS_CRUTCH = PANDAS_CRUTCH_BASE + str(id(url)) + \".txt\"\n",
        "    temp_file = open(PANDAS_CRUTCH, 'w', encoding=\"utf-8\")\n",
        "    temp_file.write(req.text)\n",
        "    temp_file.close()\n",
        "    tables = pd.read_html(PANDAS_CRUTCH)\n",
        "    table_index = 0\n",
        "\n",
        "    page_title = soup.find(\"div\", attrs={\"class\":\"title-page\"})\n",
        "    if page_title == None:\n",
        "        print(\"    ERROR: Could not find page title.\")\n",
        "        return\n",
        "\n",
        "    f = open(page_title.text.lstrip().rstrip().replace(\"\\n\",\"\") + \".txt\", 'w', encoding=\"utf-8\")\n",
        "    f.write(page_title.text.lstrip().rstrip().replace(\"\\n\",\"\") + \"\\n\")\n",
        "\n",
        "    #Required for distinguishing articles and product pages.\n",
        "    is_product_page = True\n",
        "    product_description = soup.find(\"div\", attrs={\"class\":\"product-card-dbl__col product-card-dbl__description-product\"})\n",
        "    product_card = soup.find(\"div\", attrs={\"class\":\"product-card\"})\n",
        "    if product_description == None and product_card == None:\n",
        "        is_product_page = False\n",
        "\n",
        "    if is_product_page:\n",
        "        print(\"    INFO: The page is a product page.\")\n",
        "\n",
        "        if product_description == None:\n",
        "            print(\"    INFO: Could not find the product description.\")\n",
        "        else:\n",
        "            print(\"    Parsing the product description...\")\n",
        "            for i in product_description.children:\n",
        "                if i.name == \"p\":\n",
        "                    f.write(i.text.lstrip().rstrip().replace(\"Мы предлагаем:\",\"\") + \"\\n\")\n",
        "            print(\"        Done.\")\n",
        "\n",
        "        if product_card == None:\n",
        "            print(\"    INFO: Could not find the product card.\")\n",
        "        else:\n",
        "            print(\"    Parsing the product card...\")\n",
        "            buttons = product_card.find_all(\"button\")\n",
        "            properties_tab_index = None\n",
        "            for i in range(len(buttons)):\n",
        "                if buttons[i].text.find(\"свойства\") != -1:\n",
        "                    properties_tab_index = i\n",
        "                    break\n",
        "\n",
        "            if properties_tab_index == None:\n",
        "                print(\"        INFO: Could not find the tab with the properties.\")\n",
        "            else:\n",
        "                #And another crutch. For some reason find_all() sometimes returns \n",
        "                #the parent tag as a part of the result set.\n",
        "                if recursive_call:\n",
        "                    properties_tab_index += 1\n",
        "\n",
        "                properties_tab = soup.find(\"div\", attrs={\"class\":\"product-card__tabs-content-wrap\"}).find_all(\"div\")[properties_tab_index]\n",
        "                f.write(\"Основные свойства:\\n\")\n",
        "                for i in properties_tab.children:\n",
        "                    if i.name == \"ul\":\n",
        "                        for j in i.children:\n",
        "                            if j.name == \"li\":\n",
        "                                spans = j.find_all(\"span\")\n",
        "                                f.write(spans[0].text + \": \" + spans[1].text + \"\\n\")\n",
        "                    elif i.name == \"p\":\n",
        "                        f.write(i.text + \":\\n\")\n",
        "            print(\"        Done.\")\n",
        "\n",
        "        product_assortment = soup.find(\"div\", attrs={\"class\":\"product-card-dbl__product-range-wrap\"})\n",
        "        if product_assortment == None:\n",
        "            print(\"    INFO: The page does not contain references to the product subtypes.\")\n",
        "        else:\n",
        "            print(\"    Starting the recursive traversal of the product subtypes...\\n\")\n",
        "            for i in product_assortment:\n",
        "                time.sleep(REQUEST_DELAY)\n",
        "\n",
        "                if i.name == \"a\":\n",
        "                    if i[\"href\"][0] == '/':\n",
        "                        parse_url(NIIKM_BASE_LINK[:-1] + i[\"href\"], True)\n",
        "                    else:\n",
        "                        parse_url(NIIKM_BASE_LINK + i[\"href\"], True)\n",
        "            print(\"\\n        Done.\")\n",
        "    else:\n",
        "        #TODO: add code for articles.\n",
        "        print(\"    INFO: The page is an article page. Skipping.\")\n",
        "        pass\n",
        "\n",
        "    f.close()\n",
        "    os.remove(PANDAS_CRUTCH)\n",
        "    print(\"    Done.\")\n",
        "\n",
        "for i in NIIKM_LINKS:\n",
        "    parse_url(NIIKM_BASE_LINK + i, False)\n",
        "    time.sleep(REQUEST_DELAY)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
