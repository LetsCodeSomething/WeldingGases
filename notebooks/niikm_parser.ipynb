{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kt0gjvkrw4Fb"
      },
      "outputs": [],
      "source": [
        "import collections\n",
        "from bs4 import BeautifulSoup as bs\n",
        "import os\n",
        "import pandas as pd\n",
        "import re\n",
        "import requests\n",
        "import time\n",
        "import xml.etree.ElementTree\n",
        "import shutil\n",
        "\n",
        "st_accept = \"text/html\"\n",
        "st_useragent = \"Mozilla/5.0 (Macintosh; Intel Mac OS X 12_3_1) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/15.4 Safari/605.1.15\"\n",
        "headers = {\n",
        "   \"Accept\": st_accept,\n",
        "   \"User-Agent\": st_useragent\n",
        "}\n",
        "\n",
        "NIIKM_BASE_LINK = \"https://www.niikm.ru/\"\n",
        "NIIKM_LINKS = [\"products/azot/\", \"products/argon/\", \"products/acetylene/\", \"products/hydrogen/\", \"products/helium/\",\n",
        "               \"products/carbon_dioxide/\", \"products/oxygen/\", \"products/krypton/\", \"products/xenon/\",\n",
        "               \"products/methane/\", \"products/neon/\", \"products/spbt/\"]\n",
        "REQUEST_DELAY = 3\n",
        "OUTPUT_FOLDER = \"niikm_data/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nGHxyKUfxbsg"
      },
      "outputs": [],
      "source": [
        "def parse_url(url):\n",
        "    print(\"Parsing html from \\\"\" + url + \"\\\".\")\n",
        "\n",
        "    req = requests.get(url, headers)\n",
        "\n",
        "    if req.status_code != 200:\n",
        "        print(\"    ERROR: The request status code is \" + str(req.status_code) + \".\")\n",
        "        return\n",
        "\n",
        "    soup = bs(req.text, 'lxml')\n",
        "    \n",
        "    page_title = soup.find(\"div\", attrs={\"class\":\"title-page\"})\n",
        "    if page_title == None:\n",
        "        print(\"    ERROR: Could not find page title.\")\n",
        "        return\n",
        "\n",
        "    f = open(OUTPUT_FOLDER + \"niikm_\" + page_title.text.lstrip().rstrip().replace(\"\\n\",\"\") + \".txt\", 'w', encoding=\"utf-8\")\n",
        "    f.write(\"НАЗВАНИЕ: \" + page_title.text.lstrip().rstrip().replace(\"\\n\",\"\") + \"\\n\")\n",
        "\n",
        "    #Required for distinguishing articles and product pages.\n",
        "    is_product_page = True\n",
        "    product_description = soup.find(\"div\", attrs={\"class\":\"product-card-dbl__col product-card-dbl__description-product\"})\n",
        "    product_card = soup.find(\"div\", attrs={\"class\":\"product-card\"})\n",
        "    if product_description == None and product_card == None:\n",
        "        is_product_page = False\n",
        "\n",
        "    if is_product_page:\n",
        "        print(\"    INFO: The page is a product page.\")\n",
        "\n",
        "        #Parse the state standard.\n",
        "        product_keywords = soup.find(\"p\", attrs={\"class\":\"keywords\"})\n",
        "        if product_keywords == None:\n",
        "            print(\"    INFO: Could not find the state standard.\")\n",
        "        else:\n",
        "            print(\"    Found the state standard: \" + product_keywords.text)\n",
        "            f.write(\"ГОСТ / НОРМАТИВНЫЙ ДОКУМЕНТ: \" + product_keywords.text + \"\\n\")\n",
        "\n",
        "        #Parse the product composition.\n",
        "        product_composition_table = soup.find(\"table\", attrs={\"class\":\"data-text\"})\n",
        "        if product_composition_table == None:\n",
        "            product_composition_table = soup.find(\"div\", attrs={\"class\":\"category__grid-wrap category__table-wrap\"})\n",
        "\n",
        "            if product_composition_table == None:\n",
        "                print(\"    INFO: Could not find the product composition table.\")\n",
        "            else:\n",
        "                f.write(\"ТРЕБОВАНИЯ К ПРОДУКТУ ПО ГОСТ:\\n\")\n",
        "\n",
        "                table_column_count = 0\n",
        "                table_column_index = 0\n",
        "                for i in product_composition_table.find_all(\"div\"):\n",
        "                    if i[\"class\"][0] == \"category__head-cell\":\n",
        "                        f.write(i.text.strip() + \"        \")\n",
        "                        table_column_count += 1\n",
        "                    else:\n",
        "                        if table_column_index == 0:\n",
        "                            f.write(\"\\n\")\n",
        "\n",
        "                        f.write(i.text.strip() + \"        \")\n",
        "                        table_column_index += 1\n",
        "                        if table_column_index > table_column_count - 1:\n",
        "                            table_column_index = 0\n",
        "                f.write(\"\\n\\n\")\n",
        "        else:\n",
        "            print(\"    Parsing the product composition table...\")\n",
        "            f.write(\"ТРЕБОВАНИЯ К ПРОДУКТУ ПО ГОСТ:\\n\")\n",
        "\n",
        "            #Parse the table head.\n",
        "            for i in product_composition_table.find(\"thead\").find(\"tr\").children:\n",
        "                f.write(i.text.strip() + \"        \")\n",
        "            f.write(\"\\n\")\n",
        "\n",
        "            #Parse the table content.\n",
        "            \n",
        "            #For some reason BeautifulSoup can just randomly remove tags\n",
        "            #from the received page, so we need to check if the table has\n",
        "            #tbody.\n",
        "            table_body = product_composition_table.find(\"tbody\")\n",
        "            if table_body == None:\n",
        "                #Congratulations, the table does not have tbody!\n",
        "                for i in product_composition_table.children:\n",
        "                    if i.name == \"tr\":\n",
        "                        for j in i.children:\n",
        "                            f.write(j.text.strip() + \"        \")\n",
        "                        f.write(\"\\n\")\n",
        "                f.write(\"\\n\")\n",
        "            else:\n",
        "                #Parse in the normal way.\n",
        "                table_content = table_body.find_all(\"tr\")\n",
        "                for i in table_content:\n",
        "                    for j in i.children:\n",
        "                        f.write(j.text.strip() + \"        \")\n",
        "                f.write(\"\\n\\n\")\n",
        "\n",
        "        #Parse the textual description.\n",
        "        if product_description == None:\n",
        "            print(\"    INFO: Could not find the product description.\")\n",
        "        else:\n",
        "            print(\"    Parsing the product description...\")\n",
        "            for i in product_description.children:\n",
        "                if i.name == \"p\":\n",
        "                    f.write(i.text.lstrip().rstrip().replace(\"Мы предлагаем:\",\"\") + \"\\n\")\n",
        "            print(\"        Done.\")\n",
        "\n",
        "        #Parse the detailed chemical description.\n",
        "        if product_card == None:\n",
        "            print(\"    INFO: Could not find the product card.\")\n",
        "        else:\n",
        "            print(\"    Parsing the product card...\")\n",
        "\n",
        "            tabs = soup.find(\"div\", attrs={\"class\":\"product-card__tabs-content-wrap\"}).find_all(\"div\")\n",
        "            properties_tab = None\n",
        "            for i in range(len(tabs)):\n",
        "                if str(tabs[i]).find(\"Физико-химические свойства\") != -1:\n",
        "                    properties_tab = tabs[i]\n",
        "                    break\n",
        "\n",
        "            if properties_tab == None:\n",
        "                print(\"        INFO: Could not find the tab with the properties.\")\n",
        "            else:\n",
        "                f.write(\"ОСНОВНЫЕ СВОЙСТВА:\\n\")\n",
        "                for i in properties_tab.children:\n",
        "                    if i.name == \"ul\":\n",
        "                        for j in i.children:\n",
        "                            if j.name == \"li\":\n",
        "                                spans = j.find_all(\"span\")\n",
        "                                f.write(spans[0].text + \": \" + spans[1].text + \"\\n\")\n",
        "                    elif i.name == \"p\":\n",
        "                        f.write(i.text + \":\\n\")\n",
        "            print(\"        Done.\")\n",
        "\n",
        "        #Check if the page contains references to other pages. If it does, then\n",
        "        #traverse them recursively.\n",
        "        product_assortment = soup.find(\"div\", attrs={\"class\":\"product-card-dbl__product-range-wrap\"})\n",
        "        if product_assortment == None:\n",
        "            print(\"    INFO: The page does not contain references to the product subtypes.\")\n",
        "        else:\n",
        "            print(\"    Starting the recursive traversal of the product subtypes...\\n\")\n",
        "            for i in product_assortment:\n",
        "                time.sleep(REQUEST_DELAY)\n",
        "\n",
        "                if i.name == \"a\":\n",
        "                    if i[\"href\"][0] == '/':\n",
        "                        parse_url(NIIKM_BASE_LINK[:-1] + i[\"href\"])\n",
        "                    else:\n",
        "                        parse_url(NIIKM_BASE_LINK + i[\"href\"])\n",
        "            print(\"\\n        Done.\")\n",
        "    else:\n",
        "        #TODO: add code for articles.\n",
        "        print(\"    INFO: The page is an article page. Skipping.\")\n",
        "        pass\n",
        "\n",
        "    f.close()\n",
        "    print(\"    Done.\")\n",
        "\n",
        "if os.path.isdir(OUTPUT_FOLDER[:-1]):\n",
        "    shutil.rmtree(OUTPUT_FOLDER[:-1])\n",
        "os.mkdir(OUTPUT_FOLDER[:-1])\n",
        "\n",
        "for i in NIIKM_LINKS:\n",
        "    parse_url(NIIKM_BASE_LINK + i)\n",
        "    time.sleep(REQUEST_DELAY)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!zip -r niikm_data.zip niikm_data/"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
